---
title: "Forecasting Video Games Sales"
author: Haolan Mai
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preparation and Data Cleaning

```{python}
import pandas as pd;
import numpy as np;
import seaborn as sns
from matplotlib import pyplot as plt
from matplotlib import style
from sklearn import metrics
```


### Reading and exploring data

```{python}
data = pd.read_csv('videogame.csv')
data.info()
```


```{python}
data.describe(include="all")
```

There are ~17k games, but some of the data is missing. For instance, only around half of all games has a critic score. This might be a problem for the prediction model, as critic score can be one of the major factors determing the Global Sales. User_Score has non-numeric format. Most of the games have a pretty good score, 7+ (or 70+ for critic score)

In the next cell, I am doing 4 things:

Droping games without a year of release or genre
Creating a new column for age of the game
Converting User_Score to float and replacing the tbd value in dataset with NA

```{python}
data = data.rename(columns={"Year_of_Release": "Year", 
                            "NA_Sales": "NA",
                            "EU_Sales": "EU",
                            "JP_Sales": "JP",
                            "Other_Sales": "Other",
                            "Global_Sales": "Global"})
data = data[data["Year"].notnull()]
data = data[data["Genre"].notnull()]
data["Year"] = data["Year"].apply(int)
data["Age"] = 2018 - data["Year"]
data["User_Score"] = data["User_Score"].replace("tbd", np.nan).astype(float)
data.describe(include="all")
```


From, the output above, we can see: -

There are high outliers in sales columns (NA, EU, JP, Other, Global) and User_Count column.

They might be usefull for training as they indicate bestseller games, but for now I am going to remove them and maybe add them later. The below function can be used to remove outliers present in the data set. A data entry is called an outlier if: -
value < Q1 - 3 * IQR
value > Q3 + 3 * IQR

where,

Q1 - First Quartile
Q3 - Thrid Quartile
IQR - Inter-quartile range

```{python}
def rm_outliers(df, list_of_keys):
    df_out = df
    for key in list_of_keys:
        
        # Calculate first and third quartile
        first_quartile = df_out[key].describe()["25%"]
        third_quartile = df_out[key].describe()["75%"]

        # Interquartile range
        iqr = third_quartile - first_quartile
        removed = df_out[(df_out[key] <= (first_quartile - 3 * iqr)) |
                    (df_out[key] >= (third_quartile + 3 * iqr))] 
        df_out = df_out[(df_out[key] > (first_quartile - 3 * iqr)) &
                    (df_out[key] < (third_quartile + 3 * iqr))]
        
    return df_out, removed
```


```{python}
data, rmvd_global = rm_outliers(data, ["Global"])
data.describe()
```

```{python}
data
```


There are nearly half of the games which do not have scores. In ideal cases, you would like to drop these colunmns. But dropping over 8000+ entries is not possible in our case as it will heavily affect the models. Therefore, I am going to build 2 models: a basic one and an advanced model. In a basic model I will drop games without a score (critic or user) and train it on the remaining data. I will also do minimum feature engineering or feature selection.

After I am finished with the basic model, I am going to come back to the full dataset and try to impute missing values and create new features.




# Basic Model

```{python}
# Making a new column which shows if the game is scored or not. (User score and Critic Score)

data["Has_Score"] = data["User_Score"].notnull() & data["Critic_Score"].notnull()
rmvd_global["Has_Score"] = rmvd_global["User_Score"].notnull() & rmvd_global["Critic_Score"].notnull()
```



For my basic model I am going to drop games that don't have a user score, critic score or rating. I will also remove outliers in User_Count column. Only 5.5k games, ~1/3 of all games in a dataset remaining after doing the above steps.


```{python}
scored = data.dropna(subset=["User_Score", "Critic_Score", "Rating"])
scored, rmvd_user_count = rm_outliers(scored, ["User_Count"])
scored.describe()
```


```{python}
scored["Platform"].unique(), scored["Genre"].unique(), scored["Rating"].unique()
# 17 unique platfoms, 12 unique genres and 5 ratings are present in the given data.
```


There are 17 unique platfoms, 12 unique genres and 5 ratings in the remaining data. In the advanced model I will try grouping platforms to reduce amount, but for now I will just one-hot encode them.

Features will consist of numeric columns (except for sales in regions and year - using age instead) and one-hot encoded categorical columns (platform, genre, rating).


```{python}
import category_encoders as ce

# Numeric columns
numeric_subset = scored.select_dtypes("number").drop(columns=["NA", "EU", "JP", "Other", "Year"])

# Categorical column
categorical_subset = scored[["Platform", "Genre", "Rating"]]

# One hot encoding
encoder = ce.one_hot.OneHotEncoder()
categorical_subset = encoder.fit_transform(categorical_subset)

# Column binding to the previos numeric dataset
features = pd.concat([numeric_subset, categorical_subset], axis = 1)

# Find correlations with the score 
correlations = features.corr()["Global"].dropna().sort_values()
```


Let's look at the highest and lowest correlations with the global sales column.


```{python}
correlations.head()

# Platform 5 = PC
# Genre 10 = Strategy
# Genre 12 = Adventure
# Platform 17 = PSV
# Platform 15 = PS4
```


```{python}
correlations.tail()
```

Splitting data into training set (80%) and test set (20%)

```{python}
from sklearn.model_selection import train_test_split

X = features.drop(columns="Global")
Y = pd.Series(features["Global"])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size=0.2,random_state=42)

print(X_train.shape)
```

In the next 2 cells I have: -

Defining function for mean absolute error
Defining function for fitting the model


```{python}
def mae(y_true, y_pred):
    return np.average(abs(y_true - y_pred))
```


```{python}
def fit_and_evaluate(model):
    
    # Train the model
    model.fit(X_train, Y_train)
    
    # Make predictions and evalute
    model_pred = model.predict(X_test)
    model_mae = mae(Y_test, model_pred)
    
    # Return the performance metric
    return model_mae
```


I will compare several simple models with different types of regression, and then focus on the best one for hyperparameter tuning.

```{python}
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
```


```{python}
baseline_guess = np.median(X_train)
basic_baseline_mae = mae(X_test, baseline_guess)
print("Baseline guess for global sales is: {:.02f}".format(baseline_guess))
print("Baseline Performance on the test set: MAE = {:.04f}".format(basic_baseline_mae))
```

```{python}
# Linear Regression
lr = LinearRegression()
lr_mae = fit_and_evaluate(lr)
print("Linear Regression Performance on the test set: MAE = {:.04f}".format(lr_mae))
```


```{python}
svm = SVR(C = 1000, gamma=0.1)
svm_mae = fit_and_evaluate(svm)

print("Support Vector Machine Regression Performance on the test set: MAE = {:.04f}".format(svm_mae))
```

```{python}
random_forest = RandomForestRegressor(random_state=60)
random_forest_mae = fit_and_evaluate(random_forest)
print("Random Forest Regression Performance on the test set: MAE = {:.04f}".format(random_forest_mae))
```

```{python}
gradient_boosting = GradientBoostingRegressor(random_state=60)
gradient_boosting_mae = fit_and_evaluate(gradient_boosting)
print("Gradient Boosting Regression Performance on the test set: MAE = {:.04f}".format(gradient_boosting_mae))
```


```{python}
knn = KNeighborsRegressor(n_neighbors=10)
knn_mae = fit_and_evaluate(knn)

print("K-Nearest Neighbors Regression Performance on the test set: MAE = {:.04f}".format(knn_mae))
```

```{python}
ridge = Ridge(alpha=10)
ridge_mae = fit_and_evaluate(ridge)

print("Ridge Regression Performance on the test set: MAE = {:.04f}".format(ridge_mae))
```


```{python}
MLP = MLPRegressor(hidden_layer_sizes=(10, 10, 10), max_iter=1000)
MLP_mae = fit_and_evaluate(MLP)
print("MLP Regression Performance on the test set: MAE = {:.04f}".format(MLP_mae))
```




```{python}
lasso = Lasso()
lasso_mae = fit_and_evaluate(lasso)

print("Lasso Regression Performance on the test set: MAE = {:.04f}".format(lasso_mae))
```


```{python}
style.use('ggplot')
#model_comparison = pd.DataFrame({"model": ["Linear Regression", "Support Vector Machine","Random Forest", "Gradient Boosting",
#                                            "K-Nearest Neighbors", "Ridge", "MLP Regressor", "Lasso"],
#                                 "mae": [lr_mae, svm_mae, random_forest_mae, 
#                                         gradient_boosting_mae, knn_mae, ridge_mae, MLP_mae, lasso_mae]})

model_comparison = pd.DataFrame({"model": ["Linear Regression", "Random Forest", "Gradient Boosting","MLP Regressor"],
                                 "mae": [lr_mae, random_forest_mae, gradient_boosting_mae, MLP_mae]})

model_comparison.sort_values("mae", ascending=False).plot(x="model", y="mae", kind="barh",
                                                           color="red", legend=False)
plt.ylabel(""); plt.yticks(size=14); plt.xlabel("Mean Absolute Error"); plt.xticks(size=14)
plt.title("Model Comparison on Test MAE", size=20);

# Gradient Boosting is the best out of the 5 models chosen
```

Gradient boosting regressor seems to be the best model, I will focus on this one.

First I am going to use randomized search to find the best parameters, and then I will use grid search for optimizing n_estimators.

```{python}
hyperparameter_grid = {"loss": ["ls", "lad", "huber"],
                       "max_depth": [2, 3, 5, 10, 15],
                       "min_samples_leaf": [1, 2, 4, 6, 8],
                       "min_samples_split": [2, 4, 6, 10],
                       "max_features": ["auto", "sqrt", "log2", None]}
```

```{python}
from sklearn.model_selection import RandomizedSearchCV

basic_model = GradientBoostingRegressor(random_state = 42)
random_cv = RandomizedSearchCV(estimator=basic_model,
                               param_distributions=hyperparameter_grid,
                               cv=4, n_iter=20, 
                               scoring="neg_mean_absolute_error",
                               n_jobs=-1, verbose=1, 
                               return_train_score=True,
                               random_state=42)
```


```{python}
random_cv.fit(X_train, Y_train)
```

Printing out 10 best estimators found by randomized search.

```{python}
random_results = pd.DataFrame(random_cv.cv_results_).sort_values("mean_test_score", ascending=False)
random_results.head(10)[["mean_test_score", "param_loss",
                         "param_max_depth", "param_min_samples_leaf", "param_min_samples_split",
                         "param_max_features"]]
```



```{python}
random_cv.best_estimator_
```


Using grid search to find optimal value of the n_estimators parameter.


```{python}
# Using grid search to find optimal value of the n_estimators parameter.
from sklearn.model_selection import GridSearchCV
trees_grid = {"n_estimators": [50, 100, 150, 200, 250, 300]}

basic_model = random_cv.best_estimator_
grid_search = GridSearchCV(estimator=basic_model, param_grid=trees_grid, cv=4, 
                           scoring="neg_mean_absolute_error", verbose=1,
                           n_jobs=-1, return_train_score=True)
```


```{python}
grid_search.fit(X_train, Y_train)
```


```{python}
grid_search.best_estimator_
```


```{python}
grid_search.fit(X_train, Y_train)
```

```{python}
results = pd.DataFrame(grid_search.cv_results_)

plt.plot(results["param_n_estimators"], -1 * results["mean_test_score"], label = "Testing Error")
plt.plot(results["param_n_estimators"], -1 * results["mean_train_score"], label = "Training Error")
plt.xlabel("Number of Trees"); plt.ylabel("Mean Abosolute Error"); plt.legend();
plt.title("Performance vs Number of Trees");
```

The graph shows that the model is overfitting. Training error keeps decreasing, while test error stays almost the same. It means that the model learns training examples very well, but cannot generalize on new, unknown data. This is not a very good model and try to battle overfitting in the advanced model using imputing, feature selection and feature engineering.

Let's lock the final model and see how it performs on test data.


```{python}
basic_final_model = grid_search.best_estimator_
basic_final_pred = basic_final_model.predict(X_test)
basic_final_mae = mae(Y_test, basic_final_pred)
print("Final model performance on the test set: MAE = {:.04f}.".format(basic_final_mae))
```

MAE dropped, but by a very small margin. Looks like hyperparameter tuning didn't really improve the model. I hope advanced model will have a better performance. To finish with the basic model I am going to draw 2 graphs. First one is comparison of densities of train values, test values and predictions.


```{python}
# Density plots for predictions ,test, train

sns.kdeplot(basic_final_pred, label = "Predictions")
sns.kdeplot(Y_test, label = "Test")
sns.kdeplot(Y_train, label = "Train")

plt.xlabel("Global Sales"); plt.ylabel("Density");
plt.title("Test, Train Values and Predictions");
```

Predictions density is moved a little to the right, comparing to densities of initial values. The tail is also different. This might help tuning the model in the future.

Second graph is a histogram of residuals - differences between real values and predictions.


```{python}
# Residuals plot

basic_residuals = basic_final_pred - Y_test

sns.kdeplot(basic_residuals, color = "lightskyblue")
plt.xlabel("Error"); plt.ylabel("Count")
plt.title("Distribution of Residuals")
```

# Advanced Model

```{python}
#Number of unique platforms present 
data["Platform"].unique()
```

There are too many different platforms and most of them represent a very small percent of games. I am going to group platforms to reduce the number of features.

```{python}
#Grouping platforms together
platforms = {"Playstation" : ["PS", "PS2", "PS3", "PS4"],
             "Xbox" : ["XB", "X360", "XOne"], 
             "PC" : ["PC"],
             "Nintendo" : ["Wii", "WiiU"],
             "Portable" : ["GB", "GBA", "GC", "DS", "3DS", "PSP", "PSV"]}
```

Below are the functions that I am going to use to plot the data and get inferences as well help to group the platforms as required.

```{python}
def visual_chart(column, palette="Set2"):
    values = column.value_counts().values
    labels = column.value_counts().index
    plt.pie(values, colors=sns.color_palette(palette), 
            labels=labels, autopct="%1.1f%%", 
            startangle=90, pctdistance=0.85)
    
    #draw circle
    centre_circle = plt.Circle((0,0), 0.70, fc="white")
    fig = plt.gcf()
    fig.gca().add_artist(centre_circle)
```


```{python}
def get_group_label(x, groups=None):
    if groups is None:
        return "Other"
    else:
        for key, val in groups.items():
            if x in val:
                return key
        return "Other"
```

```{python}
data["Grouped_Platform"] = data["Platform"].apply(lambda x: get_group_label(x, groups=platforms))
visual_chart(data["Grouped_Platform"])
plt.title("Groups of platforms")
plt.axis("equal");
```

Looks much better.

Now I want to check the same thing for genres.

```{python}
visual_chart(data["Genre"], palette="muted")
plt.title("Genres")
plt.axis("equal")
```

The distribution seems ok, even though there is a significant number of different genres.

```{python}
#Grouping the platforms for the entries whose score is given
scored["Grouped_Platform"] = scored["Platform"].apply(lambda x: get_group_label(x, platforms))
visual_chart(scored["Grouped_Platform"])
plt.title("Groups of platforms for games with score")
plt.axis("equal");
```


Almost all games that have scores are for "big" platfroms: PC, PS, Xbox or portable. But there are few from the "Other" group. Below are the results what the "Other" platform represents. (DC - Dreamcast)

```{python}
scored[scored["Grouped_Platform"]=="Other"]
```

Next I want to create some new features: weighted score and my own developer rating. First, I find percent of all games created by each developer, then calculate cumulative percent starting with devs with the least number of games. Finally, I divide them into 5 groups (20% each). Higher rank means more games developed.

Higher top percentage means more games developed.

```{python}
# One weighted score value including all scores and counts field.
scored["Weighted_Score"] = (scored["User_Score"] * 10 * scored["User_Count"] + 
                            scored["Critic_Score"] * scored["Critic_Count"]) / (scored["User_Count"] + scored["Critic_Count"])

# Dataframe having developers arranged based on their frequency 
devs = pd.DataFrame({"dev": scored["Developer"].value_counts().index,
                     "count": scored["Developer"].value_counts().values})

# Mean scoring datafram based on the weighted score
m_score = pd.DataFrame({"dev": scored.groupby("Developer")["Weighted_Score"].mean().index,
                        "mean_score": scored.groupby("Developer")["Weighted_Score"].mean().values})

# Creating merging the mean_score and developer dataframes and then sorting the resultant into ascending order
devs = pd.merge(devs, m_score, on="dev")
devs = devs.sort_values(by="count", ascending=True)

# Percentage of all games created by each developer and storing it in form of cumulative fashion
devs["percent"] = devs["count"] / devs["count"].sum()
devs["top%"] = devs["percent"].cumsum() * 100

# Dividing them into 10 groups
n_groups = 10
devs["top_group"] = (devs["top%"] * n_groups) // 100 + 1
devs["top_group"].iloc[-1] = n_groups
devs
```

Before creating and fitting a model I have to fill in missing values. I am filling scores and counts with zeros, because there were no real zero scores or counts in the dataset, so it will indicate absence of scores.


```{python}
data["Critic_Score"].fillna(0.0, inplace=True)
data["Critic_Count"].fillna(0.0, inplace=True)
data["User_Score"].fillna(0.0, inplace=True)
data["User_Count"].fillna(0.0, inplace=True)
data = data.join(devs.set_index("dev")["top_group"], on="Developer")
data = data.rename(columns={"top_group": "Developer_Rank"})
data["Developer_Rank"].fillna(0.0, inplace=True)
data["Rating"].fillna("None", inplace=True)
```

Removing outliers in User_Count columns.

```{python}
tmp, rmvd_tmp = rm_outliers(data[data["User_Count"] != 0], ["User_Count"])
data.drop(rmvd_tmp.index, axis=0, inplace=True)
```

Creating Weighted_Score column (earlier I did it for "scored" dataframe).


```{python}
data["Weighted_Score"] = (data["User_Score"] * 10 * data["User_Count"] + 
                            data["Critic_Score"] * data["Critic_Count"]) / (data["User_Count"] + data["Critic_Count"])
data["Weighted_Score"].fillna(0.0, inplace=True)
```

```{python}
data.info()
```

Now I will do the same things as I did in the basic model, except for using Ordinal encoding for categorical values instead of OneHot.


```{python}
# Select the numeric columns
numeric_subset = data.select_dtypes("number").drop(columns=["NA", "EU", "JP", "Other", "Year"])

# Select the categorical columns
categorical_subset = data[["Grouped_Platform", "Genre", "Rating"]]

mapping = []
for cat in categorical_subset.columns:
    tmp = scored.groupby(cat).median()["Weighted_Score"]
    mapping.append({"col": cat, "mapping": [x for x in np.argsort(tmp).items()]})
    
encoder = ce.ordinal.OrdinalEncoder()
categorical_subset = encoder.fit_transform(categorical_subset, mapping=mapping)

# Join the two dataframes using concat. Axis = 1 -> Column bind
features = pd.concat([numeric_subset, categorical_subset], axis = 1)

# Find correlations with the score 
correlations = features.corr()["Global"].dropna().sort_values()
```


```{python}
features
```


Dividing the final data into training and testing. After that I applyied the gradient boosting algorithm and then fitting the respective hyperparameters by using randomized search to do so.


```{python}
target = pd.Series(features["Global"])
features = features.drop(columns="Global")
features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2,random_state=42)
```


```{python}
model = GradientBoostingRegressor(random_state = 42)

random_cv = RandomizedSearchCV(estimator=model,
                               param_distributions=hyperparameter_grid,
                               cv=4, n_iter=20, 
                               scoring="neg_mean_absolute_error",
                               n_jobs=-1, verbose=1, 
                               return_train_score=True,
                               random_state=42)
random_cv.fit(features_train, target_train);
```

```{python}
trees_grid = {"n_estimators": [50, 100, 150, 200, 250, 300]}

model = random_cv.best_estimator_
grid_search = GridSearchCV(estimator=model, param_grid=trees_grid, cv=4, 
                           scoring="neg_mean_absolute_error", verbose=1,
                           n_jobs=-1, return_train_score=True)
grid_search.fit(features_train, target_train);
```

```{python}
# Getting the final model error 
final_model = grid_search.best_estimator_
final_pred = final_model.predict(features_test)
final_mae = mae(target_test, final_pred)
print("Final model performance on the test set: MAE = {:.04f}.".format(final_mae))
```

"Advanced" model gives better results (lower error on test set) which is a good achievement. There is definitely room for improvement. And to finish with the project, a nice group of plots summarizing the results.

```{python}
# Final Comparison Graph

plt.figure(figsize=(20, 16))
plt.title("Video Games - Predicting Global Sales", size=30, weight="bold");

ax=plt.subplot(2, 2, 1)
sns.kdeplot(final_pred, color="limegreen", label="Advanced Model")
sns.kdeplot(basic_final_pred, color="indianred", label="Basic Model")
sns.kdeplot(target_test, color="royalblue", label="Test")
plt.xlabel("Global Sales, $M", size=20)
plt.ylabel("Density", size=20)
plt.title("Distribution of Target Values", size=24)

residuals = final_pred - target_test
ax =plt.subplot(2, 2, 2)
sns.kdeplot(residuals, color = "limegreen", label="Advanced Model")
sns.kdeplot(basic_residuals, color="indianred", label="Basic Model")
plt.xlabel("Residuals, $M", size=20)
plt.ylabel("Density", size=20);
plt.title("Distribution of Errors", size=24)

feature_importance = final_model.feature_importances_
feature_names = features.columns.tolist()
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
ax =plt.subplot(2, 2, 3)
plt.barh(pos, feature_importance[sorted_idx], align='center', color="goldenrod")
plt.yticks(pos, [feature_names[x] for x in sorted_idx], size=16)
plt.xlabel('Relative Importance', size=20)
plt.title('Variable Importance', size=24)

model_comparison = pd.DataFrame({"model": ["Baseline", "Basic", "Advanced"],
                                 "mae": [basic_baseline_mae, basic_final_mae, final_mae],
                                 "color": ["royalblue", "indianred", "limegreen"]})
model_comparison.sort_values("mae", ascending=False)
pos = np.arange(3) + .5
ax =plt.subplot(2, 2, 4)
plt.barh(pos, model_comparison["mae"], align="center", color=model_comparison["color"])
plt.yticks(pos, model_comparison["model"], size=16); plt.xlabel("Mean Absolute Error", size=20);
plt.title("Test MAE", size=24)
```



